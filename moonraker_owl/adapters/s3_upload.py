"""S3 presigned URL upload client for media uploads.

This module provides an async HTTP client for uploading files to S3-compatible
storage using presigned URLs. It's used by the command processor to upload
thumbnails and other media directly to cloud storage.
"""

from __future__ import annotations

import asyncio
import logging
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Optional

import aiohttp

LOGGER = logging.getLogger(__name__)


@dataclass(slots=True)
class UploadResult:
    """Result of an S3 upload operation."""

    success: bool
    """Whether the upload succeeded."""

    s3_key: Optional[str] = None
    """The S3 key where the file was uploaded."""

    file_size_bytes: Optional[int] = None
    """Size of the uploaded file in bytes."""

    uploaded_at: Optional[datetime] = None
    """UTC timestamp when upload completed."""

    error_code: Optional[str] = None
    """Error code if upload failed."""

    error_message: Optional[str] = None
    """Human-readable error message if upload failed."""


class S3UploadClient:
    """Async client for uploading files to S3 using presigned URLs.

    This client handles direct uploads to S3-compatible storage services
    (AWS S3, MinIO, Aliyun OSS) using presigned PUT URLs generated by
    the PrinterService.

    Features:
    - Automatic retry with exponential backoff
    - Content-Type detection
    - Upload confirmation generation
    """

    def __init__(
        self,
        *,
        session: Optional[aiohttp.ClientSession] = None,
        max_retries: int = 2,
        base_retry_delay: float = 1.0,
        timeout: float = 10.0,
    ) -> None:
        """Initialize the S3 upload client.

        Args:
            session: Optional aiohttp session to use. If None, creates one.
            max_retries: Maximum number of retry attempts for failed uploads.
                Default is 2 to keep total time under 30s (capture interval).
            base_retry_delay: Base delay between retries (exponential backoff).
            timeout: Request timeout in seconds.
                Default is 10s - sufficient for typical 100-500KB images.
        """
        self._session = session
        self._owns_session = session is None
        self._max_retries = max_retries
        self._base_retry_delay = base_retry_delay
        self._timeout = timeout

    async def _ensure_session(self) -> aiohttp.ClientSession:
        """Get or create the aiohttp session."""
        if self._session is None:
            timeout = aiohttp.ClientTimeout(total=self._timeout)
            self._session = aiohttp.ClientSession(timeout=timeout)
            self._owns_session = True
        return self._session

    async def close(self) -> None:
        """Close the HTTP session if we own it."""
        if self._owns_session and self._session is not None:
            await self._session.close()
            self._session = None

    async def upload(
        self,
        presigned_url: str,
        data: bytes,
        s3_key: str,
        content_type: str = "image/png",
    ) -> UploadResult:
        """Upload data to S3 using a presigned PUT URL.

        Args:
            presigned_url: The presigned URL for uploading.
            data: Raw bytes to upload.
            s3_key: The S3 key (for result tracking, not used in upload).
            content_type: MIME type of the content.

        Returns:
            UploadResult with success status and metadata.
        """
        session = await self._ensure_session()
        file_size = len(data)

        headers = {
            "Content-Type": content_type,
            "Content-Length": str(file_size),
        }

        last_error: Optional[Exception] = None

        for attempt in range(self._max_retries):
            try:
                async with asyncio.timeout(self._timeout):
                    async with session.put(
                        presigned_url,
                        data=data,
                        headers=headers,
                    ) as response:
                        if response.status in (200, 201, 204):
                            LOGGER.info(
                                "Successfully uploaded %d bytes to S3 (key=%s)",
                                file_size,
                                s3_key,
                            )
                            return UploadResult(
                                success=True,
                                s3_key=s3_key,
                                file_size_bytes=file_size,
                                uploaded_at=datetime.now(timezone.utc),
                            )

                        # Non-retryable client errors
                        if 400 <= response.status < 500:
                            detail = await response.text()
                            LOGGER.warning(
                                "S3 upload failed with client error %d: %s",
                                response.status,
                                detail[:200],
                            )
                            return UploadResult(
                                success=False,
                                s3_key=s3_key,
                                error_code=f"http_{response.status}",
                                error_message=f"Client error: {response.status}",
                            )

                        # Server errors - retry
                        detail = await response.text()
                        last_error = RuntimeError(
                            f"S3 upload failed with status {response.status}: {detail[:200]}"
                        )
                        LOGGER.warning(
                            "S3 upload attempt %d failed with %d, retrying...",
                            attempt + 1,
                            response.status,
                        )

            except asyncio.TimeoutError as exc:
                last_error = exc
                LOGGER.warning(
                    "S3 upload attempt %d timed out after %.1fs",
                    attempt + 1,
                    self._timeout,
                )
            except aiohttp.ClientError as exc:
                last_error = exc
                LOGGER.warning(
                    "S3 upload attempt %d failed with network error: %s",
                    attempt + 1,
                    exc,
                )

            # Exponential backoff before retry
            if attempt < self._max_retries - 1:
                delay = self._base_retry_delay * (2 ** attempt)
                await asyncio.sleep(delay)

        # All retries exhausted
        error_msg = str(last_error) if last_error else "Unknown error"
        LOGGER.error(
            "S3 upload failed after %d attempts: %s",
            self._max_retries,
            error_msg,
        )
        return UploadResult(
            success=False,
            s3_key=s3_key,
            error_code="upload_failed",
            error_message=error_msg,
        )


def detect_content_type(filename: str) -> str:
    """Detect MIME type from filename extension.

    Args:
        filename: Filename or path with extension.

    Returns:
        MIME type string (defaults to application/octet-stream).
    """
    filename_lower = filename.lower()

    if filename_lower.endswith(".png"):
        return "image/png"
    if filename_lower.endswith((".jpg", ".jpeg")):
        return "image/jpeg"
    if filename_lower.endswith(".gif"):
        return "image/gif"
    if filename_lower.endswith(".webp"):
        return "image/webp"
    if filename_lower.endswith(".svg"):
        return "image/svg+xml"
    if filename_lower.endswith(".mp4"):
        return "video/mp4"
    if filename_lower.endswith(".webm"):
        return "video/webm"

    return "application/octet-stream"
